WFS theory is based on a propagation model where the acoustic field is generated by punctual primary sources and the cancellation field is generated by punctual monopole secondary sources, everything in an homogeneous media and free-space condition, and the location of every source is perfectly known.

A real situation, as the one we find in a listening room with real loudspeakers, is very different. A great variety of phenomena not contemplated by the WFS simple model occur: reflections, diffractions produced by obstacles, loudspeakers are not punctual sources so the near-field does not follow the far-field approximation, the directivity is not the one of an ideal monopole and depends on the frequency, the frequency response of loudspeakers is not flat, which would not be a problem if it was the same for every loudspeaker, but it might actually be very different from one to the other, their exact locations are not accurately known, non-linearities, etc.

All these differences influence the way the acoustic waves propagate, and in general, they worsen the performance in a real situation. Experimental measures help us understand how this not contemplated differences limit the possibilities of using WFS in the real world. In order to find a connection between the real and ideal situation, let's use a model of what is happening. In a real situation, all we can certainly know is that, if we transmit a signal through a loudspeaker and we measure at some point with a microphone, we receive a modified version of the signal. That modification depends on all the conditions previously mentioned (multiple reflections, etc.), and together they form what is often called acoustic path. An acoustic path between a loudspeaker and a point of measure acts as a filter characterized by an impulse response (time domain) or frequency response (frequency domain).

The relation between the transmitted signals and the received ones in the frequency domain is:
\begin{equation}
\Field[noValue][frequency][vector](f) = \AcPath[mat](f) \vec{\signal[nothing][frequency]}(f) ,
\end{equation}
where $\Field[noValue][frequency][vector] = [\Field[noValue][frequency][scalar]_1, \Field[noValue][frequency][scalar]_2, ..., \Field[noValue][frequency][scalar]_\numMeasPoints]^T$ ($\numMeasPoints$ is the number of points of measure) is the acoustic pressure vector, $\vec{\signal[nothing][frequency]} = [\signal[nothing][frequency]_1, \signal[nothing][frequency]_2, ..., \signal[nothing][frequency]_\numSources]^T$ ($\numSources$ is the number of loudspeakers) is the transmitted signals vector, and $\AcPath[mat]_{(\numMeasPoints \times \numSources)}$ is a matrix whose $(m,n)$-th element is the frequency response of the acoustic path between the $n$-th loudspeaker and the $m$-th point of measure.

When we differentiate the noise sources and the WFS array sources:
\begin{multline}
\Field[noValue][frequency][vector](f)
= \left. \begin{cases}
\vec{\signal[nothing][frequency]} = 
\begin{bmatrix}
\vec{\signal[wfs][frequency]} \\
\vec{\signal[ns][frequency]}
\end{bmatrix} \\
\AcPath[mat] =
\begin{bmatrix}
\AcPath[mat][WFS], & \AcPath[mat][NS]
\end{bmatrix}
\end{cases} \right\} \\
 = \AcPath[mat][NS](f) \vec{\signal[ns][frequency]}(f) + \AcPath[mat][WFS](f) \vec{\signal[wfs][frequency]}(f)
 = \Field[ns][frequency][vector](f) + \Field[wfs][frequency][vector](f),
\end{multline}
where $\Field[ns][frequency][vector] = [\Field[ns][frequency][scalar][1], \Field[ns][frequency][scalar][2], ..., \Field[ns][frequency][scalar][\numMeasPoints]]^T$, $\Field[wfs][frequency][vector] = [\Field[wfs][frequency][scalar][1], \Field[wfs][frequency][scalar][2], ..., \Field[wfs][frequency][scalar][\numMeasPoints]]^T$, $\vec{\signal[ns][frequency]} = [\signal[ns][frequency]_1, \signal[ns][frequency]_2, ..., \signal[ns][frequency]_{\numNS}]^T$ ($\numNS$ is the number of noise sources), $\vec{\signal[wfs][frequency]} = [\signal[wfs][frequency]_1, \signal[wfs][frequency]_2, ..., \signal[wfs][frequency]_{\numWFS}]^T$ ($\numWFS$ is the number of secondary sources) and $\AcPath[mat][NS]$ and $\AcPath[mat][WFS]$ are the acoustic path matrices of the noise sources and WFS secondary source array respectively. If the cancellation is successful, $\Field[ns][frequency][vector](f) \approx -\Field[wfs][frequency][vector](f)$ and so, $\Field[noValue][frequency][vector]$ becomes really small.

The signal reproduced by the secondary loudspeakers $\vec{\signal[wfs][frequency]}$ depends on the noise source signals $\vec{\signal[ns][frequency]}$. Specifically, each secondary source filters the noise source signals, as it was expressed in \autoref{rayleigh2_5Dsignal} and rewritten here:
\begin{multline}
\signal[wfs][frequency](f) = \signal[nsVirt][frequency](f) \frac{g \cos\normPrimaryPropAngleSection}{\sqrt{\distLinePrimSource}}
e^{-j k \distLinePrimSource} \sqrt{\frac{jk}{2\pi}} = \\
\left\{
\signal[nsVirt][frequency](f) = -\signal[ns][frequency] \rightarrow
H(f) = -\frac{g \cos\normPrimaryPropAngleSection}{\sqrt{\distLinePrimSource}}
e^{-j k \distLinePrimSource} \sqrt{\frac{jk}{2\pi}}
\right\} = \signal[ns][frequency](f) H(f).
\end{multline}
Expressed as a matrix multiplication:
\begin{equation}
\vec{\signal[wfs][frequency]} = \myMatrix{H} \vec{\signal[ns][frequency]},
\end{equation}
where $\myMatrix{H}_{\numWFS \times \numNS}(f)$ is a matrix where the $(m,n)$-th element is the filter frequency response that the $m$-th array loudspeaker applies to the $n$-th noise source.

In the model assumed by WFS theory (free-space conditions, ideal monopole point sources), the acoustic path response between a source and a point of measure separated by a distance $d$ is:
\begin{equation}
a(f) = \frac{e^{-j k d}}{d}.
\end{equation}
This allows us to construct $\AcPath[mat]'(f)$, the acoustic path response matrix for ideal conditions. The $(m,n)$-th element would be 
\begin{equation}
a_{m,n}'(f) = \frac{e^{-j k d_{m,n}}}{d_{m,n}},
\end{equation}
where $d_{m,n}$ is the distance between the $n$-th loudspeaker and the $m$-th point of measure.

Under these conditions, good cancellation levels can be achieved. The actual $\AcPath[mat]$ includes all sorts of variations, as we saw, so one could expect the experimental result be much worse than what theory predicts. However, a way of making this model more realistic is by adding a variable that accounts for the possible sound volume difference between the noise source and the secondary loudspeaker array. This is useful because in the listening room we generate the noise signal with a loudspeaker. Unlike the secondary array loudspeakers, which were all adjusted to have similar behaviour, the noise loudspeaker volume can be independently adjusted manually. If we change it for another loudspeaker, the volume might be actually very different. This introduces an unknown variable that definitely affects cancellation, but it's really easy to model and compensate.

Basically, regarding the volume of the source as a separate variable when dealing with real loudspeakers, force us to differentiate between the digital signal in arbitrary units $\signal[nothing][frequency]$, and the real signal in acoustic pressure units (for example, Pascals) $\soundVolume \signal[nothing][frequency]$, where $\soundVolume$ is the sound volume of the loudspeaker that transforms arbitrary units in pressure units. This term goes then included in the acoustic path matrix $\AcPath[mat]$, where the $n$-th column is multiplied by the volume associated to the $n$-th loudspeaker. Since we consider that all loudspeakers of the secondary array are well calibrated and, for simplification purposes, also considering all noise loudspeakers have the same volume:
\begin{equation}
\Field[noValue][frequency][vector]
= (\soundVolume[ns]\AcPath[mat][NS]' + \soundVolume[wfs]\AcPath[mat][WFS]'\myMatrix{H}) \vec{\signal[ns][frequency]},
\end{equation}
where $\AcPath[mat][NS]'$ and $\AcPath[mat][WFS]'$ are the ideal acoustic path matrices when the volume term is not included. This means that $\AcPath[mat][NS] = \soundVolume[ns]\AcPath[mat][NS]'$ and $\AcPath[mat][WFS] = \soundVolume[wfs]\AcPath[mat][WFS]'$.

In order to perform cancellation, we must compensate for this volume difference by multiplying the amplitude of cancellation signals by $\globalCorrectionFactor = \soundVolume[ns]/\soundVolume[wfs]$. It can be estimated after measures or if we know the acoustic path responses by finding, for example, the real number $\correctionFactor$ that optimizes next expression:
\begin{equation}
\globalCorrectionFactor = \min_\correctionFactor \norm{\Field[wfs][frequency][vector]\correctionFactor + \Field[ns][frequency][vector]}^2 = \min_\correctionFactor \norm{(\AcPath[mat][NS] + \AcPath[mat][WFS]\myMatrix{H}\correctionFactor)\vec{\signal[ns][frequency]}}^2 = -\frac{\Re\left(\scalarProd{\Field[wfs][frequency][vector]^*}{\Field[ns][frequency][vector]}\right)}{\norm{\Field[wfs][frequency][vector]}^2}
\end{equation}
In a way, this is like finding the virtual noise source signal $\vec{\signal[nsVirt][frequency]}$ that maximizes cancellation, with the restriction that it must be a scaled version of of $\vec{\signal[ns][frequency]}$.
It is not the only way we can experimentally estimate this number, but it is a simple and useful way. By compensating the volume difference, at least we can measure WFS performance as if no volume difference was present.

\section{Acoustic path frequency response estimation}
In order to measure the acoustic path frequency response, we have used a simple algorithm that can easily be implemented in Matlab.

If we were only to calculate the acoustic path between a source and one or multiple microphones, it would be actually pretty easy.

...

However, the number of sources in the listening room is 96. It is a large number, and it would be convenient to find a better, fastest way of calculating it. The technique we have used is based on orthogonal frequency division multiplexing, this is, in the fact that each loudspeaker transmits different, orthogonal frequencies that can be easily separated in the received signal.

In a communication system, a signal $x(t)$ gets transmitted through a channel with impulse response $h(t)$, and the received signal $y(t)$ is the convolution of the previous two:
\begin{equation}
y(t) = x(t)\ast h(t)
\end{equation}

Generally, in practical situations we can consider that $h(t)$ is causal and limited in time, with a duration $T_h$:
\begin{equation}
h(t) = h(t) \Pi\left(\frac{t - T_h/2}{T_h}\right)
\end{equation}
where $\Pi(t)$ is the rectangular function
\begin{equation}
\Pi(t) = \mathit{rect}(t) = \begin{cases}
0, & t < -1/2 \\
1, & -1/2 < t < 1/2 \\
0, & 1/2 < t
\end{cases}
\end{equation}

This implies that if we are only interested in knowing $y(t)$ in an interval $t \in [t_\mathit{ini}, t_\mathit{end}]$, then we just need to know $x(t)$ in the interval $t \in [t_\mathit{ini} - T_h, t_\mathit{end}]$:
\begin{equation}
y(t) = \Pi\left(\frac{t - (t_\mathit{end} + t_\mathit{ini} - T_h)/2}{t_\mathit{end} - (t_\mathit{ini} - T_h)}\right) x(t) \ast h(t), \quad t_\mathit{ini} <= t <= t_\mathit{end}.
\end{equation}

The spectrum of that interval of the received signal will be
\begin{multline}
Y'(f) = \FourierTransform{y'(t)} = \FourierTransform{y(t) \Pi\left(\frac{t - (t_\mathit{end} + t_\mathit{end})/2}{t_\mathit{end} - t_\mathit{ini}}\right)} = \\ Y(f) \ast (t_\mathit{end} - t_\mathit{ini}) \mathit{sinc}\left((t_\mathit{end} - t_\mathit{ini}) f \right) e^{-j 2 \pi f \frac{(t_\mathit{end} + t_\mathit{ini})}{2}},
\end{multline}
where $\mathit{sinc}$ is the function:
\begin{equation}
\mathit{sinc}(x) = \frac{\sin(\pi x)}{\pi x}
\end{equation}

Let's assume that we are transmitting some tones with equispaced frequencies $f_k = k \Delta f$, where $k = -N - 1, ..., 0, ..., N-1$ ($x(t)$ is real). As a consequence, if the duration of the selected interval of $y(t)$ is multiple of $1/\Delta f$, this is, $T = t_\mathit{end} - t_\mathit{ini} = n/\Delta f$ where $n = 1, 2, ...$, then the zeroes of the $\mathit{sinc}$ function fall on multiples of $1/T = \Delta f / n$, meaning that 
\begin{equation}
Y'(f_k) = Y(f_k), \quad f = k \Delta f.
\end{equation}

The values of $H(f)$ can be known with precision at those frequencies ($f = k \Delta f$) by dividing the received spectrum by the sent one:
\begin{equation}
H(f_k) = \frac{Y'(f_k)}{X(f_k)}, f_k = k \Delta f
\label{Hestimate1}
\end{equation}

The value of $H(f)$ at the rest of frequencies is unknown. Nonetheless, it can be deduced if some additional conditions are satisfied. Let's notice that discretizing a spectrum with a base separation $\Delta f_0$ is equivalent to adding a periodicity in the time domain:
\begin{equation}
H_\delta(f) = H(f) \Delta f \sum_{k = -\infty}^{\infty} \delta(f - k \Delta f) = \FourierTransform{h(t) \ast \sum_{n = -\infty}^{\infty} \delta\left(t - \frac{n}{\Delta f}\right)}[inverse] = \FourierTransform{h_\delta(t)}[inverse],
\end{equation}
where $H_\delta(f)$ is the discretized channel frequency response.

If the period is bigger than the duration of the impulse response ($1/\Delta f >= T_h$), then the frequency discretization will not produce any interference, and so, no information will be lost. In that case, the spectrum of the impulse response can be calculated by just convolution a $\mathit{sinc}$ function by the known frequencies:
\begin{equation}
h(t) = h_\delta(t) \Pi\left( \frac{t - \frac{1}{2\Delta f}}{1/\Delta f} \right) \leftrightarrow
H(f) = H_\delta(f) \ast \frac{1}{\Delta f} \mathit{sinc}\left( \frac{1}{\Delta f} f \right) e^{-j 2 \pi f \frac{1}{2 \Delta f}}
\end{equation}

However, there is still the question of calculating $H_\delta(f)$. One could say that, just using \autoref{Hestimate1} is enough. However, let's notice that, in order to known the value of the channel response at frequency $f_k$, the transmitted signal should have a tone at that frequency. But, realistically $x(t)$ is limited in frequency ($f_k = k\Delta f$ and $k_{max} = N - 1$, so $f_\mathit{max} = (N-1)\Delta f$. So, we can't actually know $H_\delta(t)$ but a frequency limited version:
\begin{equation}
H_\delta^{'}(f) = H_\delta(f) \Pi\left(\frac{f}{2 f_\mathit{max}}\right) \leftrightarrow h_\delta^{'}(t) = h_\delta \ast 2f_\mathit{max} \mathit{sinc}\left(  2f_\mathit{max} t \right)
\label{frequencyCut}
\end{equation}

The estimated channel response will become
\begin{equation}
h_e(f) = h_\delta^{'} \Pi\left( \frac{t - \frac{1}{2\Delta f}}{1/\Delta f} \right) \leftrightarrow
H_e(f) = H_\delta^{'}(f) \ast \frac{1}{\Delta f} \mathit{sinc}\left( \frac{1}{\Delta f} f \right) e^{-j 2 \pi f \frac{1}{2 \Delta f}}
\end{equation}

The estimated channel response will only be equal to the real one if, in addition to the conditions already mentioned ($1/\Delta f \geq T_h$ and $T = n/\Delta f$, $n = 1, 2, ...$), the maximum frequency of the impulse response $f_{h,max}$ is smaller than the maximum transmitted frequency: $f_{h,max} \leq f_{max}$.

If previous condition is not satisfied, equality cannot be guaranteed. The estimation will get better as $\Delta f$ decreases. It can be understood intuitively as making the period of the repetition of $h(t)$ bigger, so the convolution of other periods with the $\mathit{sinc}$ (\autoref{frequencyCut}) does not influence a lot the main one because the distance is very big ($1/\Delta f$) and the contribution "arrives" attenuated.

A question that may arise is how can we transmit pure tones if we can only generate discretized finite tones that are converted to continuous signal by the digital to analogue converter of the loudspeaker.
...

In summary, the process I propose is:
\begin{itemize}
	\item Transmit signals and record them
	\item Select an interval with the right duration
	\item Calculate discretized spectrum: Calculate response at key frequencies
	\item Convolute by adequate sincs in order to calculate the full estimated spectrum
\end{itemize}

\section{Measures}
In order to perform a simple experiment, a loudspeaker will act as a noise source that transmits a known signal. Specifically, it is a chirp signal of duration $4$ seconds. The frequency increases linearly from $20 \si{Hz}$ to $950\si{Hz}$ (\autoref{NSsignal}). We have chosen to work at a sample rate of $11025$.
\begin{figure}[h]
	\begin{subfigure}[b]{0.49\textwidth}
	\centering
	\def\svgwidth{0.9\columnwidth}
	\graphicspath{{Img/}}
	{\fontsize{5}{12}\selectfont
		\input{Img/Experiment16_NSsignalTime.pdf_tex}
	}
	\caption{Time}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
	\centering
	\def\svgwidth{0.9\columnwidth}
	\graphicspath{{Img/}}
	{\fontsize{5}{12}\selectfont
		\input{Img/Experiment16_NSsignalFreq.pdf_tex}
	}
	\caption{Frequency}
\end{subfigure}
	\caption{Noise source signal}
	\label{NSsignal}
\end{figure}
%The received signal from that noise loudspeaker is shown in \autoref{recNS}
%\begin{figure}
%	\begin{subfigure}[b]{0.49\textwidth}
%		\centering
%		\def\svgwidth{0.9\columnwidth}
%		\graphicspath{{Img/}}
%		{\fontsize{5}{12}\selectfont
%			\input{Img/Experiment16_recNSTime.pdf_tex}
%		}
%		\caption{Time}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.49\textwidth}
%		\centering
%		\def\svgwidth{0.9\columnwidth}
%		\graphicspath{{Img/}}
%		{\fontsize{5}{12}\selectfont
%			\input{Img/Experiment16_recNSFreq.pdf_tex}
%		}
%		\caption{Frequency}
%	\end{subfigure}
%	\caption{Received signal from noise source}
%	\label{recNS}
%\end{figure}

The signal transmitted by the WFS array loudspeakers $\vec{\signal[wfs][time]} = [\signal[wfs][time]_1, \signal[wfs][time]_2, ..., \signal[wfs][time]_{\numWFS}]^T$ are calculated using \autoref{rayleigh2_5Dsignal}, where the filter $\freqFilter(t) = \FourierTransform{\sqrt{\frac{jk}{2\pi}}}[inverse]$ has been implemented using a magnitude filter $\freqFilter[time][magnitude] = \FourierTransform{\sqrt{f/c}}[inverse]$ of order $1024$, and a phase filter $\freqFilter[time][phase] = \FourierTransform{\sqrt{j}}[inverse]$ of order $4096$. The estimated noise source position (the positions that is used to calculate WFS signals) is $\Position[ns] = [\Position[ns][x], \Position[ns][y], \Position[ns][z]] = [3.5, -0.2, 1.65]$ (remember all loudspeakers are situated $1.65\si{m}$ above the floor). 
When playing the noise source and WFS signals simultaneously, the received signals (in comparison with the ones received only from the noise source) are shown in \autoref{recSign}.
\begin{figure}[h]
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\def\svgwidth{0.9\columnwidth}
		\graphicspath{{Img/}}
		{\fontsize{5}{12}\selectfont
			\input{Img/Experiment16_recAndrecNStime_1.pdf_tex}
		}
		\caption{Microphone 1}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\def\svgwidth{0.9\columnwidth}
		\graphicspath{{Img/}}
		{\fontsize{5}{12}\selectfont
			\input{Img/Experiment16_recAndrecNStime_2.pdf_tex}
		}
		\caption{Microphone 2}
	\end{subfigure}
	\caption{Received signal}
	\label{recSign}
\end{figure}

The average gain with and without volume correction is shown in \autoref{averGain}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\columnwidth]{Img/Experiment16_averGainComparison.eps}
	\caption{Average gain comparison}
	\label{averGain}
\end{figure}

\section{Justification for optimization}
The extent of the optimization depends on what we want to test. If we only wanted to test how much unpredictable variables affect performance, counting volume disparity as one of those variables, it would not make sense to perform an optimization, because it would be falsifying the results. However, if we consider the volume disparity as a problem inherent to the listening room situation and not a general one, or that in a more real situation it can be solved, or optimized in real-time, then it makes sense to optimize. Hence, it is important to define what we want to test.

We have considered that, in order to fairly assess WFS performance, the volume disparity shouldn't be something the WFS technique must solve. We want to know its possibilities given that the strength of the noise source is known, this is, that the system of microphones that measure noise strength, no matter they are situated close to the secondary sources or to the noise source (depending on the architecture of the actual setup), provides the WFS algorithm with a good estimation of signal strength. So it is fair, and even necessary, to correct for volume disparity in the laboratory, since we assume that in the real situation it would be a solved problem. Even if it is not, in a real situation we can also optimize a posteriori based on measures in the sound cancellation area. Anyway one wants to describe a real situation, the point is that WFS technique is not responsible for wrong estimations of primary source signal strength; that is a problem that must be solved with other techniques available. The described optimization is, indeed, one of those techniques.

Other variables as reflections, diffractions, etc., shouldn't be compensated because there is actually where we can see the effectiveness of WFS in front of real situations where acoustic paths are unpredictable. For example, achieving silence on a small number of microphones is easy by solving the linear system, but it's not WFS, it's a completely different technique that does not guarantee sound cancellation over the whole area.